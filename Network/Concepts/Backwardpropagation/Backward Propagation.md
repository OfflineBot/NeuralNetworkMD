# Backward Propagation
First calculate the Gradient Descent, then update the Weights and Biases.
Calculate the [[Gradient Descent|Gradients]].
The use the [[Optimization]] Algorithm.

## Example:
SGD:
$\theta_{new} = \theta_{old} - \eta * gradient$

Where:
- $\theta_{new}$: new parameter
- $\theta_{old}$: current parameter
- $\eta$: learning rate
- $gradient$: gradient of parameter


# Stochastic Gradient Descent

$\theta_{new} = \theta_{old} - \eta * gradient$

Where:
- $\theta_{new}$: new parameter
- $\theta_{old}$: current parameter
- $\eta$: learning rate
- $gradient$: gradient of parameter
